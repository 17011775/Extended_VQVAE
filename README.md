# VC_VQVAE
This is a Pytorch implementation of extended VQVAE mentioned in [our paper](https://arxiv.org/abs/2005.07884).

We reconstructed the speech using both [original VQVAE](https://arxiv.org/abs/1711.00937) and [F0 encoder](https://arxiv.org/abs/2005.07884). 

# Samples
Please find our samples [here](https://nii-yamagishilab.github.io/yi-demo/interspeech-2020/index.html).

# Usage

# Trained models
1. Japanese : [original VQVAE]  [extended VQVAE]
2. Chinese: original VQVAE: [checkpoints/ch_vqvae.43.upconv.pyt](https://github.com/nii-yamagishilab/VC_VQVAE/blob/master/checkpoints/ch_vqvae.43.upconv.pyt)       
            extended VQVAE:[checkpoints/ch_vqvae.43.upconv.pyt](https://github.com/nii-yamagishilab/VC_VQVAE/blob/master/checkpoints/ch_vcf0.43.upconv.pyt)      


# Acknowledgement

The code is based on [mkotha/WaveRNN](https://github.com/mkotha/WaveRNN)
